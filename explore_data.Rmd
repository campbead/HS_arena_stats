---
title: "Hearthstone Arena Statistic Exploration"
output: html_notebook
---

Hearthstone just released some official Arena card data.  I think this is super cool since I don't know they have every fully released this data to the public.  The community has been asking for some official data for some time. 

https://playhearthstone.com/en-gb/blog/21964711/developer-insights-12-0-arena-update-with-kris-zierhut

let's load the data and have a look.

```{r}
library(dplyr)
library(ggplot2)
appearence_data <- read.csv('data/Appearance_Rates.csv')
```

Ok so the first thing we might do is see what the top 10 cards are in each class

```{r}
top_10_in_each_class <- appearence_data %>%
  group_by(Draft.Class) %>%
  top_n(10, Average)
```

So that gives us top 10 cards by average occurances/draft for each class.  I had to look up that function `top_n`.  Let's get a little more infomation on the function.

```{r}
help("top_n")
```

So pretty great function, it also can order by ascending by using a minus `-` flag.  It says that the function is a wrapper for using the `filter()` and the `min_rank()` functions.

Let's look at just the top single card for each class

```{r}
top_card_each_class <- appearence_data %>%
  group_by(Draft.Class) %>%
  top_n(1, Average)

print(top_card_each_class)
```
These are all either common or basic cards and with the exception of Warrior, they are all Spells.  

Well maybe the next thing to do is to take a look at the least common for each class:
```{r}
lowest_card_each_class <- appearence_data %>%
  group_by(Draft.Class) %>%
  top_n(-1, Average)

print(lowest_card_each_class)
```
Holy Millhouse Cho. Hearthstone doesn't offer Millhouse Manastorm or Lorewalker Cho very often.  Now my next question is how big is that spread?
```{r}
class_spreads_high<- top_card_each_class %>%
  select(Average)
class_spreads_low <- lowest_card_each_class %>%
  select(Average)
class_spreads <- class_spreads_high
class_spreads$Low_average <- class_spreads_low$Average

class_spreads$ratio <- class_spreads$Average / class_spreads$Low_average
```
Ok so these range from about 400 - 300 times 

I'm wondering what the shape of those likelihood looks like.  Linearly? Decaying? Stepped?

That'll be my next plot.  This will be a lot like the Rooms ordering from my Legend of Zelda analysis Check it out if you're interested, [LoZscraper](https://github.com/campbead/LoZscraper).



```{r}
stripped_data <- appearence_data %>%
  group_by(Draft.Class) %>%
  select(Draft.Class,Average)

stripped_data <- stripped_data %>%
  group_by(Draft.Class) %>%
  mutate(ranking = rank(-Average,ties.method = 'first'))
```
Wow that did EXACTLY what I wanted.
```{r}
ggplot(data = stripped_data, aes(x=ranking, y=Average)) +
 geom_point(aes(color = Draft.Class))
```
That worked perfectly.  This is really cool figure showing the shape of the likelihood of each card.  What we can see is overall shape is of a decaying exponential, but there are some finer structures like plateaus.  I suspect these plateaus are related to the buckets, which is in the other dataset.  Let's a quick look at a single Class
```{r}
# filter out Priest data
Priest_rankings <- stripped_data %>%
  filter(Draft.Class == 'Priest')

# plot priest data
ggplot(data = Priest_rankings, aes(x=ranking, y=Average)) +
 geom_point(color = 'pink',size = 0.5)
```
Looking at this figure it looks less exponenetial with the cards offered most often.  There is a pretty clear break after card 20 or so.  Let's explore that in a bit more depth.
```{r}
# plot priest data zoomed in 
ggplot(data = Priest_rankings, aes(x=ranking, y=Average)) +
 geom_point(color = 'pink',size = 0.5) + 
  coord_cartesian(xlim =c(0,50))
```
From here it appears like the 15-18 most common cards are really in a league of their own.  

I'm wondering what that looks like in a CDF.  
```{r}
# plot priest data zoomed in 
ggplot(data = Priest_rankings, aes(x=Average)) +
  stat_ecdf(pad = FALSE)
```
This `stat_ecdf` function doesn't do exactly what i'm looking for since it only works on one axis.  

I think this fixes it
```{r}
# use the cumsum function to calcu a cumulative sum on Average, then divide by 90 to normalize
Priest_rankings$cumsum <- cumsum(Priest_rankings$Average)/90

# plot
ggplot(data = Priest_rankings, aes(x=ranking, y=cumsum)) +
 geom_point(color = 'pink',size = 0.5) +
  ylab('cummulative probablity') +
  xlab('card rank number')
```
Ok so there are some interesting features here.  63 of 463 cards are roughly half of those seen in drafts.  Each draft you'll see 90 cards, so 45 of those cards seen will selected from just 63 cards.  The half of cards (231) are the least likely to appear, will only appear less than 8% of the time.  So of the 90 cards you see in a draft about 7 of them will be from the lowest likelihood half.  An interesting insight.

